{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TCA+\n",
    "from tl_algs import tca_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old REPD model\n",
    "from REPD_Impl import REPD\n",
    "from autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performance metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Result presentation\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Other\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_arff(dataset_name, data_preparation_function):\n",
    "    #Load data\n",
    "    data, _ = arff.loadarff(\"./data/\"+dataset+\".arff\")\n",
    "    \n",
    "     # Wrap data into a pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    #Prepare data\n",
    "    df = data_preparation_function(df)\n",
    "    \n",
    "    #Return dataframe\n",
    "    return df\n",
    "\n",
    "def load_csv(dataset_name, data_preparation_function):\n",
    "    #Load data\n",
    "    data = pd.read_csv(\"./data/\"+dataset_name+\".csv\")\n",
    "    \n",
    "    #Prepare data\n",
    "    df = data_preparation_function(data)\n",
    "    \n",
    "    #Return dataframe\n",
    "    return df\n",
    "\n",
    "def load_data(dataset_name, dataset_settings):\n",
    "    if dataset_settings[\"type\"] == \"arff\":\n",
    "        return load_arff(dataset_name, dataset_settings[\"prep_func\"])\n",
    "    elif dataset_settings[\"type\"] == \"csv\":\n",
    "        return load_csv(dataset_name, dataset_settings[\"prep_func\"])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_defect_0_1_prep_func(defect_column_name, mapping_function):\n",
    "    #\n",
    "    def defect_0_1_prep_func(df):\n",
    "        df[defect_column_name] = df[defect_column_name].map(mapping_function)\n",
    "        return df\n",
    "    #\n",
    "    return defect_0_1_prep_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cleanup_prep_func_decorator(prep_func):\n",
    "    #\n",
    "    def decorated_prep_func_1(df):\n",
    "        df = prep_func(df)\n",
    "        \n",
    "        #Remove all with missing values\n",
    "        df = df.dropna()\n",
    "\n",
    "        #Remove duplicate instances\n",
    "        df = df.drop_duplicates()\n",
    "        #\n",
    "        return df\n",
    "    #\n",
    "    return decorated_prep_func_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_adjust_defect_column_name_prep_func_decorator(defecive_column_name, prep_func):\n",
    "    #\n",
    "    def decorated_prep_func_2(df):\n",
    "        df = prep_func(df)\n",
    "        \n",
    "        #Rename column\n",
    "        df = df.rename(columns={defecive_column_name: \"defective\"})\n",
    "        #\n",
    "        return df\n",
    "    #\n",
    "    return decorated_prep_func_2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_remove_column_prep_func_decorator(columns_to_remove, prep_func):\n",
    "    def decorated_prep_func_3(df):\n",
    "        df = prep_func(df)\n",
    "        \n",
    "        #Drop columns\n",
    "        df = df.drop(columns=columns_to_remove)\n",
    "        #\n",
    "        return df\n",
    "    #\n",
    "    return decorated_prep_func_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class REPD_EX:\n",
    "    \n",
    "    def __init__(self, base_repd, defective_classification_model, non_defective_classification_model, use_def_m=True, use_non_def_m=True):\n",
    "        self.base_repd = base_repd\n",
    "        self.defective_classification_model = defective_classification_model\n",
    "        self.non_defective_classification_model = non_defective_classification_model\n",
    "        self.use_def_m=use_def_m\n",
    "        self.use_non_def_m=use_non_def_m\n",
    "    \n",
    "    def fit(self, X, y, train_base=True):\n",
    "        #\n",
    "        if train_base:\n",
    "            X_m1_fit, X_m2_fit, y_m1_fit, y_m2_fit = train_test_split(X, y, test_size=0.5)\n",
    "            #\n",
    "            oversample = SMOTE()\n",
    "            X_m1_fit, y_m1_fit = oversample.fit_resample(X_m1_fit, y_m1_fit)\n",
    "            #\n",
    "            self.base_repd.fit(X_m1_fit,y_m1_fit)\n",
    "        else:\n",
    "            X_m2_fit = X\n",
    "            y_m2_fit = y\n",
    "        #\n",
    "        y_p = self.base_repd.predict(X_m2_fit)\n",
    "        X_r = pd.DataFrame(self.base_repd.transform(X_m2_fit))\n",
    "        #\n",
    "        #========================================\n",
    "        if self.use_def_m:\n",
    "            X_s_o_t = X_m2_fit[y_p==1]\n",
    "            X_s_r_t = X_r[y_p==1]\n",
    "            #\n",
    "            X_s_o_t = pd.DataFrame(X_s_o_t.values)\n",
    "            X_s_r_t = pd.DataFrame(X_s_r_t.values)\n",
    "            #\n",
    "            X_s_t = pd.concat([X_s_o_t, X_s_r_t], axis=1, join=\"inner\")\n",
    "            #\n",
    "            y_s_t = y_m2_fit[y_p==1]\n",
    "            #\n",
    "            #print(\"P Defective:\",len(y_s_t[y_s_t==1]),\"/\",len(y_s_t))\n",
    "            #\n",
    "            oversample = SMOTE()\n",
    "            X_s_t, y_s_t = oversample.fit_resample(X_s_t, y_s_t)\n",
    "            #\n",
    "            self.defective_classification_model.fit(X_s_t, y_s_t)\n",
    "        #========================================\n",
    "        if self.use_non_def_m:\n",
    "            X_s_o_t = X_m2_fit[y_p==0]\n",
    "            X_s_r_t = X_r[y_p==0]\n",
    "            #\n",
    "            X_s_o_t = pd.DataFrame(X_s_o_t.values)\n",
    "            X_s_r_t = pd.DataFrame(X_s_r_t.values)\n",
    "            #\n",
    "            X_s_t = pd.concat([X_s_o_t, X_s_r_t], axis=1, join=\"inner\")\n",
    "            #\n",
    "            y_s_t = y_m2_fit[y_p==0]\n",
    "            #\n",
    "            #print(\"P Defective:\",len(y_s_t[y_s_t==1]),\"/\",len(y_s_t))\n",
    "            #\n",
    "            oversample = SMOTE()\n",
    "            X_s_t, y_s_t = oversample.fit_resample(X_s_t, y_s_t)\n",
    "            #\n",
    "            self.non_defective_classification_model.fit(X_s_t, y_s_t)\n",
    "        #========================================\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_p = self.base_repd.predict(X)\n",
    "        #\n",
    "        X_r = pd.DataFrame(self.base_repd.transform(X))\n",
    "        #\n",
    "        #========================================\n",
    "        if self.use_def_m:\n",
    "            X_s_o_t = X[y_p==1]\n",
    "            X_s_r_t = X_r[y_p==1]\n",
    "            #\n",
    "            X_s_o_t = pd.DataFrame(X_s_o_t.values)\n",
    "            X_s_r_t = pd.DataFrame(X_s_r_t.values)\n",
    "            #\n",
    "            X_s_t = pd.concat([X_s_o_t, X_s_r_t], axis=1, join=\"inner\")\n",
    "            #\n",
    "            y_s_p_p = self.defective_classification_model.predict(X_s_t)\n",
    "        #========================================\n",
    "        if self.use_non_def_m:\n",
    "            X_s_o_t = X[y_p==0]\n",
    "            X_s_r_t = X_r[y_p==0]\n",
    "            #\n",
    "            X_s_o_t = pd.DataFrame(X_s_o_t.values)\n",
    "            X_s_r_t = pd.DataFrame(X_s_r_t.values)\n",
    "            #\n",
    "            X_s_t = pd.concat([X_s_o_t, X_s_r_t], axis=1, join=\"inner\")\n",
    "            #\n",
    "            y_s_n_p = self.non_defective_classification_model.predict(X_s_t)\n",
    "        #========================================\n",
    "        r = []\n",
    "        #\n",
    "        cnt1 = 0\n",
    "        k1 = 0\n",
    "        cnt2 = 0\n",
    "        k2 = 0\n",
    "        for v in y_p:\n",
    "            if v == 0:\n",
    "                if self.use_non_def_m:\n",
    "                    if hasattr(y_s_n_p[k2], \"__len__\"):\n",
    "                        nmr = y_s_n_p[k2][0]\n",
    "                    else:\n",
    "                        nmr = y_s_n_p[k2]\n",
    "                    if nmr >= 0.5:\n",
    "                        r.append(1)\n",
    "                    else:\n",
    "                        r.append(0)\n",
    "                        cnt2 = cnt2 + 1\n",
    "                    k2 = k2 + 1\n",
    "                else:\n",
    "                    r.append(0)\n",
    "            else:\n",
    "                if self.use_def_m:\n",
    "                    if hasattr(y_s_p_p[k1], \"__len__\"):\n",
    "                        nmr = y_s_p_p[k1][0]\n",
    "                    else:\n",
    "                        nmr = y_s_p_p[k1]\n",
    "                    if nmr >= 0.5:\n",
    "                        r.append(1)\n",
    "                        cnt1 = cnt1 + 1\n",
    "                    else:\n",
    "                        r.append(0)\n",
    "                    k1 = k1 + 1\n",
    "                else:\n",
    "                    r.append(1)\n",
    "        #\n",
    "        #print(\"S:\", len(X), \"P:\", len(y_s_p_p), \"C1:\", cnt1, \"N:\", len(y_s_n_p), \"C2:\", cnt2)\n",
    "        #\n",
    "        return y_p, np.asarray(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_results(y_true,y_predicted):\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_predicted, average='binary')\n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_res(column):\n",
    "    bp_dict = results_df.boxplot(\n",
    "        column=column,\n",
    "        by=[\"Model\"],\n",
    "        layout=(1,1),       \n",
    "        return_type='both',\n",
    "        patch_artist = True,\n",
    "        vert=False,\n",
    "    )    \n",
    "    plt.suptitle(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_datasets = [\n",
    "    \"JDT_R2_0\",\"JDT_R2_1\",\"JDT_R3_0\",\"JDT_R3_1\",\"JDT_R3_2\"\n",
    "]\n",
    "target_datasets = [\n",
    "    \"PDE_R2_0\",\"PDE_R2_1\",\"PDE_R3_0\",\"PDE_R3_1\",\"PDE_R3_2\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_settings = {\n",
    "  \"JDT_R2_0\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                      get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                          get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                    ))\n",
    "                  )\n",
    "  },\n",
    "  \"JDT_R2_1\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                      get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                          get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                    ))\n",
    "                  )      \n",
    "  },\n",
    "  \"JDT_R3_0\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                          get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                              get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                      )\n",
    "                    )\n",
    "                  )      \n",
    "  },\n",
    "  \"JDT_R3_1\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                          get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                              get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                      )\n",
    "                    )\n",
    "                  )\n",
    "      \n",
    "  },\n",
    "  \"JDT_R3_2\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                          get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                              get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                        )\n",
    "                    )\n",
    "                  )      \n",
    "  },\n",
    "  \"PDE_R2_0\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                      get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                          get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                    ))\n",
    "                  )\n",
    "  },\n",
    "  \"PDE_R2_1\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                      get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                          get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                    ))\n",
    "                  )      \n",
    "  },\n",
    "  \"PDE_R3_0\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                          get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                              get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                      )\n",
    "                    )\n",
    "                  )      \n",
    "  },\n",
    "  \"PDE_R3_1\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                          get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                              get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                      )\n",
    "                    )\n",
    "                  )\n",
    "      \n",
    "  },\n",
    "  \"PDE_R3_2\": {\n",
    "      \"type\":\"csv\",\n",
    "      \"prep_func\": get_cleanup_prep_func_decorator(\n",
    "                      get_remove_column_prep_func_decorator( [\"File\"],\n",
    "                          get_adjust_defect_column_name_prep_func_decorator(\"bug_cnt\",\n",
    "                              get_defect_0_1_prep_func(\"bug_cnt\", lambda x: 1 if x>0 else 0)\n",
    "                        )\n",
    "                    )\n",
    "                  )      \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_data = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datasets, datasets_type in [(source_datasets,\"source\"),(target_datasets,\"target\")]:\n",
    "    for dataset in datasets:\n",
    "        print(\"Loading dataset: \", dataset)\n",
    "        df = load_data(dataset,dataset_settings[dataset])\n",
    "        #\n",
    "        dataset_data[dataset] = {\n",
    "            \"df\": df,\n",
    "            \"X\": df.drop(columns=[\"defective\"]),\n",
    "            \"y\": df[\"defective\"],\n",
    "            \"share\": (len(df[df[\"defective\"]==1])/ len(df)),\n",
    "            \"type\": datasets_type\n",
    "        }\n",
    "        #\n",
    "        print(\"Defective:\", len(df[df[\"defective\"]==1]), \"/\", len(df),\"=\",(len(df[df[\"defective\"]==1])/ len(df)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REPETITION_COUNT = 100\n",
    "TEST_SIZE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in dataset_data:\n",
    "    dataset_data[dataset][\"train\"] = []\n",
    "    dataset_data[dataset][\"test\"] = []\n",
    "    # \n",
    "    X = dataset_data[dataset][\"X\"]\n",
    "    y = dataset_data[dataset][\"y\"]\n",
    "    #\n",
    "    for _ in range(REPETITION_COUNT):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "        #\n",
    "        dataset_data[dataset][\"train\"].append((X_train, y_train))\n",
    "        dataset_data[dataset][\"test\"].append((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_train(X_train):\n",
    "    train_min = X_train.min()\n",
    "    train_dim = X_train.max() - X_train.min()\n",
    "    train_dim[train_dim == 0] = 1\n",
    "    #\n",
    "    X_train = (X_train - train_min) / train_dim\n",
    "    #\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    #\n",
    "    return X_train, train_min, train_dim\n",
    "\n",
    "def normalize_test(X_test, train_min, train_dim):\n",
    "    X_test = (X_test - train_min) / train_dim\n",
    "    #\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    #\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder([48,24],0.01,100,50)\n",
    "for mode, cls_w_c1, cls_w_c2, use_c1, use_c2 in [\n",
    "                                (\"balance\",{0:1., 1: 2.},{0:2., 1: 1.},True, True), \n",
    "                                (\"hp\",{0:1.2, 1: 1.},{0:1., 1: 1.},True,False)\n",
    "                                ]:\n",
    "    for source in dataset_data:\n",
    "        #\n",
    "        if dataset_data[source][\"type\"] != \"source\":\n",
    "            continue\n",
    "        #\n",
    "        X_train = dataset_data[source][\"X\"]\n",
    "        y_train = dataset_data[source][\"y\"]\n",
    "        #\n",
    "        X_train, train_min, train_dim = normalize_train(X_train)\n",
    "        #\n",
    "        for target in dataset_data:\n",
    "            if target == source:\n",
    "                continue\n",
    "            #\n",
    "            if dataset_data[target][\"type\"] != \"target\":\n",
    "                continue\n",
    "            #\n",
    "            print(\"Mode:\",mode,\" Source:\",source,\" Target:\",target,\" Share:\", dataset_data[target][\"share\"])\n",
    "            #\n",
    "            performance_data = []\n",
    "            #\n",
    "            for i in range(REPETITION_COUNT):\n",
    "                if (i+1)%2 ==0:\n",
    "                    print(\"Repetition: \", (i+1))\n",
    "                #============================================================\n",
    "                try:\n",
    "                    X_target_train, y_target_train = dataset_data[target][\"train\"][i]\n",
    "                    X_test, y_test = dataset_data[target][\"test\"][i]\n",
    "                    #\n",
    "                    X_target_train = normalize_test(X_target_train, train_min, train_dim)\n",
    "                    X_test = normalize_test(X_test, train_min, train_dim)\n",
    "                    #============================================================\n",
    "                    X_train_join = pd.concat([X_train, X_target_train], axis=0, join=\"inner\").reset_index(drop=True)\n",
    "                    y_train_join = pd.concat([y_train, y_target_train], axis=0, join=\"inner\").reset_index(drop=True)\n",
    "                    #============================================================\n",
    "                    train_pool_domain = [1 if e<len(X_train) else 0 for e in range(len(X_train_join))]\n",
    "                except:\n",
    "                    print(\"Error while preparing data\")\n",
    "                    continue\n",
    "                #============================================================\n",
    "                print(\"TCA+\")\n",
    "                try:\n",
    "                    _tca = tca_plus.TCAPlus(\n",
    "                        test_set_domain = 0,\n",
    "                        train_pool_domain = train_pool_domain,\n",
    "                        test_set_X = X_test, \n",
    "                        train_pool_X = X_train_join, \n",
    "                        train_pool_y = y_train_join, \n",
    "                        Base_Classifier = LogisticRegression\n",
    "                    )            \n",
    "                    confidence, y_p = _tca.train_filter_test() \n",
    "                    accuracy, precision, recall, f1_score = calculate_results(y_test, y_p)\n",
    "\n",
    "                    #Store results\n",
    "                    data = ['TCA+', accuracy, precision, recall, f1_score, source, target]\n",
    "                    performance_data.append(data)\n",
    "                except:\n",
    "                    print(\"Error while running TCA+\")\n",
    "                    continue\n",
    "\n",
    "                #============================================================\n",
    "                try:\n",
    "                    X_m1_fit, X_m2_fit, y_m1_fit, y_m2_fit = train_test_split(X_train, y_train, test_size=0.8)\n",
    "                    #============================================================\n",
    "                    #REPD\n",
    "                    print(\"REPD\")\n",
    "                    autoencoder.re_init()\n",
    "                    classifer = REPD(autoencoder)\n",
    "                    classifer.fit(X_m1_fit, y_m1_fit)\n",
    "                    y_p = classifer.predict(X_test)\n",
    "                    accuracy, precision, recall, f1_score = calculate_results(y_test, y_p)\n",
    "\n",
    "                    #Store results\n",
    "                    data = ['REPD', accuracy, precision, recall, f1_score, source, target]\n",
    "                    performance_data.append(data)\n",
    "\n",
    "                    #REPD_EX\n",
    "                    print(\"REPDX\")\n",
    "                    final_classifier_1 = LogisticRegression(class_weight=cls_w_c1)\n",
    "                    #\n",
    "                    final_classifier_2 =  LogisticRegression(class_weight=cls_w_c2)\n",
    "                    #\n",
    "                    classifer = REPD_EX(base_repd=classifer, \n",
    "                                        defective_classification_model=final_classifier_1,\n",
    "                                        non_defective_classification_model=final_classifier_2,\n",
    "                                        use_def_m=use_c1,\n",
    "                                        use_non_def_m=use_c2)\n",
    "                    classifer.fit(X_m2_fit, y_m2_fit, train_base=False)\n",
    "                    #\n",
    "                    y_pb, y_p = classifer.predict(X_test)\n",
    "                    accuracy_base, precision_base, recall_base, f1_base_score = calculate_results(y_test, y_pb)\n",
    "                    accuracy, precision, recall, f1_score = calculate_results(y_test, y_p)\n",
    "\n",
    "                    #Store results\n",
    "                    data = ['REPD_EX', accuracy, precision, recall, f1_score, source, target]\n",
    "                    performance_data.append(data)\n",
    "                except:\n",
    "                    print(\"Error while running REPD and REPDX\")\n",
    "                    continue\n",
    "            results_df = pd.DataFrame(performance_data, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Source', 'Target'])\n",
    "            results_df.to_csv(\"results_v2/cross_\"+mode+\"_\"+source+\"_\"+target)\n",
    "            #\n",
    "            print(\"Median:\", results_df.groupby([\"Model\"])[\"F1 score\", 'Precision', 'Recall'].median())    \n",
    "            print()\n",
    "            print()\n",
    "autoencoder.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
